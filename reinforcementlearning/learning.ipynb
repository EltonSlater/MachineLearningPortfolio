{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>reinforcement learning</h1>\n",
    "<p>this project uses reinforcement learning in order to teach an ai to balance a stick on a swivel</p>\n",
    "\n",
    "<p>reinforcement learning works by first making random choices and if those choices work towards the goal the ai is rewarded if they don't it isnt rewared the ai then attempts to use what it has learned while also trying new aproaches and once again is either rewarded or not based of the results, this repeats for as many times as you allow it untill it has learned how to do it's task</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classical gym \n",
    "import gym\n",
    "# instead of gym, import gymnasium \n",
    "#import gymnasium as gym\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment\n",
    "env=gym.make('CartPole-v1',render_mode='human')\n",
    "# reset the environment, \n",
    "# returns an initial state\n",
    "(state,_)=env.reset()\n",
    "# states are\n",
    "# cart position, cart velocity \n",
    "# pole angle, pole angular velocity\n",
    "\n",
    "# render the environment\n",
    "env.render()\n",
    "# close the environment\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push cart in one direction\n",
    "env.step(0)\n",
    "\n",
    "# observation space limits\n",
    "env.observation_space\n",
    " \n",
    "# upper limit\n",
    "env.observation_space.high\n",
    " \n",
    "# lower limit\n",
    "env.observation_space.low\n",
    " \n",
    " \n",
    "# action space\n",
    "env.action_space\n",
    " \n",
    "# all the specs\n",
    "env.spec\n",
    " \n",
    "# maximum number of steps per episode\n",
    "env.spec.max_episode_steps\n",
    " \n",
    "# reward threshold per episode\n",
    "env.spec.reward_threshold\n",
    "\n",
    "# simulate the environment\n",
    "episodeNumber=1000\n",
    "timeSteps=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or episodeIndex in range(episodeNumber):\n",
    "    initial_state=env.reset()\n",
    "    print(episodeIndex)\n",
    "    env.render()\n",
    "    appendedObservations=[]\n",
    "    for timeIndex in range(timeSteps):\n",
    "        print(timeIndex)\n",
    "        random_action=env.action_space.sample()\n",
    "        observation, reward, terminated, truncated, info =env.step(random_action)\n",
    "        appendedObservations.append(observation)\n",
    "        #time.sleep(0.001)\n",
    "        if (terminated):\n",
    "            time.sleep(.001)\n",
    "            break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
